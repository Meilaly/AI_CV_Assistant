# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
from peft import PeftModel
import json
import pandas as pd
import docx  # ç”¨äºå¤„ç†.docxæ–‡ä»¶
from pdfplumber import open as pdf_open  # ç”¨äºå¤„ç†.pdfæ–‡ä»¶
from pptx import Presentation  # ç”¨äºå¤„ç†.pptxæ–‡ä»¶

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'
lora_path = './output/Yuan2.0-2B_lora_bf16/checkpoint-51'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10
# torch_dtype = torch.float16 # P100

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creat tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
                          '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>',
                          '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)

    print("Creat model...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = AutoModelForCausalLM.from_pretrained(
        path,
        torch_dtype=torch.bfloat16 if device.type == 'cuda' else torch.float32,
        trust_remote_code=True
    )
    model.to(device)  # ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šçš„è®¾å¤‡
    model = PeftModel.from_pretrained(model, model_id=lora_path)

    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()


template = '''
# ä»»åŠ¡æè¿°
å‡è®¾ä½ æ˜¯ä¸€ä¸ªAIç®€å†åŠ©æ‰‹ï¼Œèƒ½ä»ç®€å†ä¸­è¯†åˆ«å‡ºæ‰€æœ‰çš„å‘½åå®ä½“ï¼Œå¹¶ä»¥jsonæ ¼å¼è¿”å›ç»“æœã€‚

# ä»»åŠ¡è¦æ±‚
å®ä½“çš„ç±»åˆ«åŒ…æ‹¬ï¼šå§“åã€å›½ç±ã€ç§æ—ã€èŒä½ã€æ•™è‚²èƒŒæ™¯ã€ä¸“ä¸šã€ç»„ç»‡åã€åœ°åã€‚
è¿”å›çš„jsonæ ¼å¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­æ¯ä¸ªé”®æ˜¯å®ä½“çš„ç±»åˆ«ï¼Œå€¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å®ä½“çš„æ–‡æœ¬ã€‚

# æ ·ä¾‹
è¾“å…¥ï¼š
å¼ ä¸‰ï¼Œç”·ï¼Œä¸­å›½ç±ï¼Œå·¥ç¨‹å¸ˆ
è¾“å‡ºï¼š
{"å§“å": ["å¼ ä¸‰"], "å›½ç±": ["ä¸­å›½"], "èŒä½": ["å·¥ç¨‹å¸ˆ"]}

# å½“å‰ç®€å†
query

# ä»»åŠ¡é‡è¿°
è¯·å‚è€ƒæ ·ä¾‹ï¼ŒæŒ‰ç…§ä»»åŠ¡è¦æ±‚ï¼Œè¯†åˆ«å‡ºå½“å‰ç®€å†ä¸­æ‰€æœ‰çš„å‘½åå®ä½“ï¼Œå¹¶ä»¥jsonæ ¼å¼è¿”å›ç»“æœã€‚
'''
# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Yuan2.0 AIç®€å†åŠ©æ‰‹")

# åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
st.write("è¯·ä¸Šä¼ ç®€å†æ–‡ä»¶ï¼š")

# å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["pdf", "docx", "pptx", "txt"])

# åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
st.chat_message("assistant").write(f"è¯·è¾“å…¥ç®€å†æ–‡æœ¬ï¼š")

# è¯»å–æ–‡ä»¶çš„å‡½æ•°
def read_file(file):
    if file.type == 'application/pdf':
        with pdf_open(file) as pdf:
            text = '\n'.join(page.extract_text() for page in pdf.pages)
    elif file.type.startswith('application/vnd.openxmlformats-officedocument'):
        if file.name.endswith('.docx'):
            doc = docx.Document(file)
            text = '\n'.join([p.text for p in doc.paragraphs])
        elif file.name.endswith('.pptx'):
            prs = Presentation(file)
            text = ''
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + '\n'
    elif file.type == 'text/plain':
        text = file.read().decode('utf-8')
    else:
        raise ValueError("Unsupported file type. Please upload a PDF, DOCX, PPTX, or TXT file.")

    return text

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if query := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(query)

    # ä½¿ç”¨st.empty()åˆ›å»ºä¸€ä¸ªå¯æ›´æ–°çš„åŒºåŸŸ
    empty_placeholder = st.empty()

    # åœ¨è¯¥åŒºåŸŸå†…æ˜¾ç¤ºâ€œæ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...â€
    with empty_placeholder.container():
        st.write("æ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...")

    # è°ƒç”¨æ¨¡å‹
    prompt = template.replace('query', query).strip()
    prompt += "<sep>"
    inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
    outputs = model.generate(inputs, do_sample=False, max_length=1024)  # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '').strip()

    # æ¸…ç©ºâ€œæ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...â€çš„ä¿¡æ¯
    empty_placeholder.empty()

    # åˆ›å»ºä¸€ä¸ªæ–°çš„st.chat_messageå¹¶æ˜¾ç¤ºè§£æåçš„è¡¨æ ¼
    new_placeholder = st.chat_message("assistant")
    with new_placeholder.container():
        df = pd.DataFrame(json.loads(response))
        st.table(df)

if uploaded_file is not None:
    # è¯»å–æ–‡ä»¶å†…å®¹
    query = read_file(uploaded_file)

    # æ˜¾ç¤ºä¸Šä¼ çš„æ–‡ä»¶å†…å®¹
    st.chat_message("user").write(query)

    # ä½¿ç”¨st.empty()åˆ›å»ºä¸€ä¸ªå¯æ›´æ–°çš„åŒºåŸŸ
    empty_placeholder = st.empty()

    # åœ¨è¯¥åŒºåŸŸå†…æ˜¾ç¤ºâ€œæ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...â€
    with empty_placeholder.container():
        st.write("æ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...")

    try:
        # å‡†å¤‡prompt
        prompt = template.format(query=query).strip()
        prompt += "<sep>"
        inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
        max_new_tokens = 1024 - len(inputs[0])  # ç¡®ä¿æ€»é•¿åº¦ä¸è¶…è¿‡1024
        outputs = model.generate(inputs, do_sample=False, max_new_tokens=max_new_tokens)  # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
        output = tokenizer.decode(outputs[0])
        response = output.split("<sep>")[-1].replace("<eod>", '').strip()

        # å¤„ç†é•¿åº¦ä¸åŒçš„åˆ—è¡¨
        parsed_response = json.loads(response)
        max_length = max(len(entities) for entities in parsed_response.values())
        for key in parsed_response:
            parsed_response[key] = parsed_response[key] + [''] * (max_length - len(parsed_response[key]))

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(parsed_response)

        # æ¸…ç©ºâ€œæ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...â€çš„ä¿¡æ¯
        empty_placeholder.empty()

        # åˆ›å»ºä¸€ä¸ªæ–°çš„st.chat_messageå¹¶æ˜¾ç¤ºè§£æåçš„è¡¨æ ¼
        new_placeholder = st.chat_message("assistant")
        with new_placeholder.container():
            st.table(df)
    except Exception as e:
        # æ¸…ç©ºâ€œæ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...â€çš„ä¿¡æ¯
        empty_placeholder.empty()

        # åˆ›å»ºä¸€ä¸ªæ–°çš„st.chat_messageå¹¶æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        new_placeholder = st.chat_message("assistant")
        with new_placeholder.container():
            st.error(f"å‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯ï¼š{str(e)}")

# å¦‚æœæ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œåˆ™æ˜¾ç¤ºæç¤ºä¿¡æ¯
else:
    st.info("è¯·ä¸Šä¼ ä¸€ä»½ç®€å†æ–‡ä»¶ä»¥ä¾¿å¼€å§‹åˆ†æã€‚")